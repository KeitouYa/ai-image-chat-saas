/**
 * Performance Metrics Collection Script
 * Runs actual requests and collects real performance metrics:
 * - p95 latency
 * - cache hit rate
 * - fallback rate
 * - error rate
 */

import { sendChatMessage } from "@/src/services/chat.service";
import { logger } from "@/src/lib/logger";

interface PerformanceMetrics {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  cachedRequests: number;
  fallbackRequests: number;
  latencies: number[];
  cachedLatencies: number[];
  uncachedLatencies: number[];
}

interface MetricsResult {
  cacheHitRate: number;
  fallbackRate: number;
  errorRate: number;
  p95Latency: number;
  p95CachedLatency: number;
  p95UncachedLatency: number;
  avgLatency: number;
  minLatency: number;
  maxLatency: number;
}

// Test messages - mix of common (will be cached) and unique
const TEST_MESSAGES = [
  // Common messages (will hit cache after first request)
  "What is React?",
  "Explain TypeScript",
  "What is Next.js?",
  "How does caching work?",
  "What is MongoDB?",
  
  // Unique messages (won't hit cache)
  `Explain React hooks at ${Date.now()}`,
  `What is async programming? ${Math.random()}`,
  `Tell me about TypeScript generics ${Date.now()}`,
  `How does Redis work? ${Math.random()}`,
  `Explain API design ${Date.now()}`,
];

function calculatePercentile(sortedArray: number[], percentile: number): number {
  const index = Math.ceil((percentile / 100) * sortedArray.length) - 1;
  return sortedArray[Math.max(0, index)] || 0;
}

async function runPerformanceTest(
  numRequests: number = 20,
  userId: string = "test-user-metrics"
): Promise<MetricsResult> {
  const metrics: PerformanceMetrics = {
    totalRequests: 0,
    successfulRequests: 0,
    failedRequests: 0,
    cachedRequests: 0,
    fallbackRequests: 0,
    latencies: [],
    cachedLatencies: [],
    uncachedLatencies: [],
  };

  console.log(`\nðŸ§ª Running performance test: ${numRequests} requests...\n`);

  // Run requests
  for (let i = 0; i < numRequests; i++) {
    const message = TEST_MESSAGES[i % TEST_MESSAGES.length];
    const requestId = `perf-test-${i}-${Date.now()}`;
    const startTime = Date.now();

    try {
      // Enable metrics logging temporarily
      process.env.METRICS_ENABLED = "true";

      const result = await sendChatMessage(
        message,
        "gemini",
        userId,
        requestId
      );

      const latency = Date.now() - startTime;
      metrics.totalRequests++;
      metrics.successfulRequests++;
      metrics.latencies.push(latency);

      if (result.cached) {
        metrics.cachedRequests++;
        metrics.cachedLatencies.push(latency);
        console.log(`âœ… Request ${i + 1}: CACHED (${latency}ms)`);
      } else {
        metrics.uncachedLatencies.push(latency);
        console.log(`âœ… Request ${i + 1}: UNCACHED (${latency}ms)`);
      }

      // Small delay to avoid rate limiting
      await new Promise((resolve) => setTimeout(resolve, 500));
    } catch (error) {
      const latency = Date.now() - startTime;
      metrics.totalRequests++;
      metrics.failedRequests++;
      metrics.latencies.push(latency);
      console.log(`âŒ Request ${i + 1}: FAILED (${latency}ms) - ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // Calculate metrics
  const sortedLatencies = [...metrics.latencies].sort((a, b) => a - b);
  const sortedCached = [...metrics.cachedLatencies].sort((a, b) => a - b);
  const sortedUncached = [...metrics.uncachedLatencies].sort((a, b) => a - b);

  const result: MetricsResult = {
    cacheHitRate: metrics.totalRequests > 0 
      ? (metrics.cachedRequests / metrics.totalRequests) * 100 
      : 0,
    fallbackRate: 0, // Would need to parse logs or add tracking
    errorRate: metrics.totalRequests > 0 
      ? (metrics.failedRequests / metrics.totalRequests) * 100 
      : 0,
    p95Latency: calculatePercentile(sortedLatencies, 95),
    p95CachedLatency: sortedCached.length > 0 
      ? calculatePercentile(sortedCached, 95) 
      : 0,
    p95UncachedLatency: sortedUncached.length > 0 
      ? calculatePercentile(sortedUncached, 95) 
      : 0,
    avgLatency: sortedLatencies.length > 0
      ? sortedLatencies.reduce((a, b) => a + b, 0) / sortedLatencies.length
      : 0,
    minLatency: sortedLatencies.length > 0 ? sortedLatencies[0] : 0,
    maxLatency: sortedLatencies.length > 0 ? sortedLatencies[sortedLatencies.length - 1] : 0,
  };

  return result;
}

// Parse metrics from logs (if METRICS_ENABLED was true)
async function parseMetricsFromLogs(): Promise<{
  fallbackRate: number;
  avgLatencies: { cached: number; uncached: number };
}> {
  // This would parse actual log files or use a log aggregation system
  // For now, return placeholder - in production would parse structured logs
  return {
    fallbackRate: 0,
    avgLatencies: { cached: 0, uncached: 0 },
  };
}

async function main() {
  try {
    console.log("=".repeat(60));
    console.log("  PERFORMANCE METRICS COLLECTION");
    console.log("=".repeat(60));

    const numRequests = parseInt(process.argv[2]) || 20;
    const userId = process.argv[3] || "test-user-metrics";

    const metrics = await runPerformanceTest(numRequests, userId);

    console.log("\n" + "=".repeat(60));
    console.log("  RESULTS");
    console.log("=".repeat(60));
    console.log(`\nðŸ“Š Cache Hit Rate:     ${metrics.cacheHitRate.toFixed(1)}%`);
    console.log(`ðŸ“Š Error Rate:          ${metrics.errorRate.toFixed(1)}%`);
    console.log(`\nâ±ï¸  Average Latency:     ${metrics.avgLatency.toFixed(0)}ms`);
    console.log(`â±ï¸  P95 Latency:          ${metrics.p95Latency.toFixed(0)}ms`);
    console.log(`â±ï¸  P95 Cached:           ${metrics.p95CachedLatency.toFixed(0)}ms`);
    console.log(`â±ï¸  P95 Uncached:         ${metrics.p95UncachedLatency.toFixed(0)}ms`);
    console.log(`â±ï¸  Min Latency:          ${metrics.minLatency.toFixed(0)}ms`);
    console.log(`â±ï¸  Max Latency:          ${metrics.maxLatency.toFixed(0)}ms`);

    // Save to file
    const fs = require("fs");
    const path = require("path");
    const reportPath = path.join(__dirname, "..", "performance-metrics.json");
    
    const report = {
      timestamp: new Date().toISOString(),
      testConfig: {
        numRequests,
        userId,
      },
      metrics,
    };

    fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));
    console.log(`\nðŸ“„ Report saved to: performance-metrics.json`);

    // Resume-friendly summary
    console.log("\n" + "=".repeat(60));
    console.log("  RESUME METRICS SUMMARY");
    console.log("=".repeat(60));
    console.log(`\nâœ… Cache Hit Rate: ${metrics.cacheHitRate.toFixed(1)}%`);
    console.log(`âœ… P95 Latency (Cached): ${metrics.p95CachedLatency.toFixed(0)}ms`);
    console.log(`âœ… P95 Latency (Uncached): ${metrics.p95UncachedLatency.toFixed(0)}ms`);
    console.log(`âœ… Error Rate: ${metrics.errorRate.toFixed(1)}%`);

    if (metrics.cacheHitRate >= 30 && metrics.p95CachedLatency < 1000 && metrics.p95UncachedLatency < 3000) {
      console.log("\nðŸŽ‰ Metrics look great for resume!");
    }

  } catch (error) {
    console.error("Error collecting performance metrics:", error);
    process.exit(1);
  } finally {
    // Close DB connection if needed
    const mongoose = require("mongoose");
    if (mongoose.connection.readyState === 1) {
      await mongoose.connection.close();
    }
  }
}

if (require.main === module) {
  main();
}

export { runPerformanceTest };
export type { MetricsResult };

